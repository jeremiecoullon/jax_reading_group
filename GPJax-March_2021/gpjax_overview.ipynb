{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GPJax Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Process Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Maths recap\n",
    "\n",
    "Consider data $(X, f, y)=\\{x_i, f_i, y_i\\}^N_{i=1}$ where $x_i\\in\\mathbb{R}^d$, and $y_i \\in \\mathbb{R}$ is a stochastic observation depending on $f_i=f(x_i)$ for some latent function $f$. Let $k_{\\theta}(x, x')$ be a positive definite kernel function parameterised by a set of hyperparameters $\\theta$ with resultant Gram matrix $K_{xx}=k_{\\theta}(x, x')$. Following standard practice in the literature and assuming a zero mean function, we can posit the hierarchical GP framework as \n",
    "$$p(y \\lvert f , \\theta) = \\prod_{i=1}^N p(y_i \\lvert f_i, \\theta)  f\\lvert X, \\qquad \\theta \\sim \\mathcal{N}(0, K_{xx}), \\qquad \\theta \\sim p_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the above generative model, we can see that the posterior distribution of a GP is\n",
    "$$p(f, \\theta \\lvert y) = \\frac{1}{C}p(y \\lvert f,\\theta) p(f \\lvert \\theta)p_0(\\theta)$$\n",
    "where $C$ denotes the unknown normalisation constant of the posterior. Often we are interested in using the posterior to make new function predictions $f^{\\star}$ for test data $X^{\\star}$,\n",
    "$$p\\left(f^{\\star} | y\\right)=\\iint p\\left(f^{\\star} | f, \\theta\\right) p(f, \\theta | y) \\,\\mathrm{d} \\theta \\,\\mathrm{d} f$$\n",
    "\n",
    "When the likelihood $p(y_i\\lvert f_i,\\theta)$ is Gaussian the posterior predictive distribution conditional on $\\theta$ is analytically available as we can marginalise $f$ out from the predcitive distribution, and inference methods focus on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Demo](intro_to_gps.jl) (note: this is a [Pluto](https://github.com/fonsp/Pluto.jl) notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPJax overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aims of GPJax\n",
    "\n",
    "* Provide a GP API that represents the underlying maths\n",
    "* Provide a low-level API for research purposes\n",
    "* Give me a good excuse to play around in Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principles\n",
    "\n",
    "* _Functional_ in design\n",
    "    * Multiple dispatch helps with this\n",
    "    * There appears to be some synergy between Jax and multiple dispatching\n",
    "        * It's not perfect though...\n",
    "* Try not to hide any GP trickery\n",
    "* Fancy way to make Gaussian random variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Documentation source](https://gpjax.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Abstraction choice\n",
    "\n",
    "A Jax API with multiple dispatch makes choices around how much of the API to expose really simple.\n",
    "\n",
    "Almost everything is a function, so a more heavily abstracted API can be made by simply piping one function through another. This is achieved without having to remember multiple function names.\n",
    "\n",
    "For example, in GPJax, the posterior GP is represented by a Gaussian random variable. For people just wanting to fit a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "Working with Jax's AutoDiff module means and optimisation steps can be made verbose. \n",
    "\n",
    "```python\n",
    "mll = jit(marginal_ll(posterior, negative=True))\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=0.01)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "def step(i, opt_state):\n",
    "    p = get_params(opt_state)\n",
    "    g = jax.grad(mll)(p, x, y)\n",
    "    return opt_update(i, g, opt_state)\n",
    "\n",
    "[opt_state, mll_estimate = step(i, opt_state) for i in range(100)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the aim of GPJax is to simply provide the building blocks for building GPs, this means we don't have to wrap the optimisation up in a mysterious `posterior.fit()` style method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Wider ecosystem\n",
    "\n",
    "* GPJax heavily uses Chex and TensorFlow Probability\n",
    "* Both packages are almost seamless to compose with Jax\n",
    "\n",
    "#### Chex\n",
    "\n",
    "Provides a _struct-like_ backbone to the code e.g., [GPs](https://github.com/thomaspinder/GPJax/blob/master/gpjax/gps.py) and [Kernels](https://github.com/thomaspinder/GPJax/blob/master/gpjax/kernels/base.py)\n",
    "\n",
    "#### TensorFlow Probability\n",
    "\n",
    "Provides a clean way to state distributions e.g., [Priors](https://gpjax.readthedocs.io/en/latest/nbs/tfp_interface.html#State-priors) and also to return them e.g., [GP random variables](https://gpjax.readthedocs.io/en/latest/nbs/regression.html#Realising-the-random-variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing\n",
    "\n",
    "Jax, Chex and PyTest together make writing unit tests really easy. \n",
    "\n",
    "Chex has some handy Jax-specific unit test functions. Straight out of the Chex README:\n",
    "```python\n",
    "assert_tree_all_close(tree_x, tree_y)  # values and structure of trees match\n",
    "assert_tree_all_finite(tree_x)         # all tree_x leaves are finite\n",
    "```\n",
    "\n",
    "Similar to PyTest parameterisations, Chex has some decorator functions:\n",
    "```python\n",
    "@chex.variants(with_jit=True, without_jit=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The bad\n",
    "\n",
    "Disclaimer: None of these packages/approaches are explicitly bad, they just were not great for GPJax's usage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ObJax\n",
    "\n",
    "* V0.1-0.2 of GPJax used ObJax to enable a more modular code base.\n",
    "* Unlike in Jax, quantities are stateful in ObJax.\n",
    "\n",
    "* Unfortunately, anything new in Jax/Jax's ecosystem takes a while to propogate into ObJax e.g., [Optax](https://github.com/deepmind/optax) and [Elegy](https://github.com/poets-ai/elegy).\n",
    "* Gradients, particularly second-order, are tricky/messy to compute in ObJax as they're w.r.t the object's `.vars()`.\n",
    "    * This was particularly problematic for implementing HMC and Laplace posterior approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modularity\n",
    "\n",
    "Currently the only modularity in GPJax is provided through Chex's `dataclass` objects. The entire conjugate posterior is\n",
    "```python\n",
    "@dataclass\n",
    "class ConjugatePosterior:\n",
    "    prior: Prior\n",
    "    likelihood: Gaussian\n",
    "    name: Optional[str] = \"ConjugatePosterior\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        meanf_string = self.prior.mean_function.__repr__()\n",
    "        kernel_string = self.prior.kernel.__repr__()\n",
    "        likelihood_string = self.likelihood.__repr__()\n",
    "        return f\"Conjugate Posterior\\n{'-'*80}\\n- {meanf_string}\\n- {kernel_string}\\n- {likelihood_string}\"\n",
    "```\n",
    "\n",
    "I'm unsure how annoying/undesirable this is to all users(?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parameter transforms\n",
    "\n",
    "Different parameters have different constraints, if any at all.\n",
    "\n",
    "The way this is currently handled is to for the user to define some parameters and a corresponding transform e.g.,\n",
    "```python\n",
    "params = {\"lengthscale\": jnp.array([1.0]), \"variance\": jnp.array([1.0]), \"obs_noise\": jnp.array([1.0])}\n",
    "params = transform(params, SoftplusTransformation)\n",
    "# Do you optimisation...\n",
    "final_params = untransform(params, SoftplusTransformation)\n",
    "```\n",
    "\n",
    "It's fragile and inelgant though e.g., [ignoring unconstrained parameter](https://github.com/thomaspinder/GPJax/blob/master/gpjax/parameters/transforms.py#L8).\n",
    "\n",
    "It'd be nice to have a parameter-transformation lookup, but it's hard to see how to make this work in Jax without defining custom gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The ugly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parameter handling\n",
    "\n",
    "#### Trade off \n",
    "\n",
    "1. __Dictionaries__ give a clean and safe interface. \n",
    "2. Less value corcion is need with __arrays__.\n",
    "\n",
    "#### Current approach\n",
    "\n",
    "All parameters, regardless of whether they're fixed or trainable, are dictionaries.  \n",
    "\n",
    "This does make the underlying code more readable e.g. for scaling an input by a lengthscale parameter\n",
    "\n",
    "```python\n",
    "def scale(x: Array, params: dict):\n",
    "    return x/params['lengthscale']\n",
    "```\n",
    "vs.\n",
    "```python\n",
    "def scale(x: Array, params: Array):\n",
    "    return x/params[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parameter handling\n",
    "\n",
    "Coercion is a pain though... \n",
    "\n",
    "For example, integration with TFP's HMC sampler parameters must be an array:\n",
    "\n",
    "```python\n",
    "def array_to_dict(varray: jnp.DeviceArray, keys: List):\n",
    "    pdict = {}\n",
    "    for val, key in zip(varray, keys):\n",
    "        pdict[key] = val\n",
    "    return pdict\n",
    "\n",
    "\n",
    "def build_log_pi(params, target_fn):\n",
    "    param_keys = list(params.keys())\n",
    "\n",
    "    def target(params: jnp.DeviceArray):\n",
    "        coerced_params = array_to_dict(params, param_keys)\n",
    "        return target_fn(coerced_params)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "mll = marginal_ll(posterior, negative=False)\n",
    "target_dist = partial(mll, x=x, y=y, priors=priors)\n",
    "log_pi = build_log_pi(params, target_dist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parameter handling\n",
    "\n",
    "Jax also sorts dictionaries whenever it operates on them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': DeviceArray(4., dtype=float32), 'b': array(0., dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "\n",
    "parameters = {\"b\": jnp.array(1.0), \"a\": jnp.array(2.0)}\n",
    "\n",
    "def f(params: dict) -> jnp.DeviceArray:\n",
    "    return parameters[\"b\"] * jnp.square(params[\"a\"])\n",
    "\n",
    "print(grad(f)(parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parameter handling\n",
    "\n",
    "Although safer in a dictionary, parameters stored in this way are also more fragile. For example, this would break GPJax\n",
    "```\n",
    "params = {\n",
    "    \"lenghtscale\": jnp.array([1.0]),\n",
    "    \"variance\": jnp.array([1.0]),\n",
    "    \"obs_noise\": jnp.array([1.0]),\n",
    "}\n",
    "```\n",
    "and it's not immediate obvious why...\n",
    "\n",
    "GPJax does provide some initialisers to try and mitigate this imperfection - [example](https://gpjax.readthedocs.io/en/latest/nbs/regression.html#Stating-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiple dispatch isn't perfect (yet)\n",
    "\n",
    "Dispatch types must be explicitly stated e.g., \n",
    "```python\n",
    "@dispatch(jnp.DeviceArray, SpectralKernel, int, int)\n",
    "def sample_frequencies(\n",
    "    key, kernel: SpectralKernel, n_frequencies: int, input_dimension: int\n",
    ") -> jnp.DeviceArray:\n",
    "    density = spectral_density(kernel)\n",
    "    return density.sample(sample_shape=(n_frequencies, input_dimension), seed=key)\n",
    "```\n",
    "\n",
    "It would be cleaner if the dispatch decorator was just\n",
    "```python\n",
    "@dispatch\n",
    "def sample_frequencies(\n",
    "    key, kernel: SpectralKernel, n_frequencies: int, input_dimension: int\n",
    ") -> jnp.DeviceArray:\n",
    "```\n",
    "and types inferred from the function's typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiple dispatch isn't perfect (yet)\n",
    "\n",
    "Arrays are not the easiest type to dispatch on in Jax.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax.interpreters.xla._DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "def f(x: jnp.DeviceArray):\n",
    "    print(type(x))\n",
    "    return jnp.square(x)\n",
    "\n",
    "x = jnp.array(1.)\n",
    "y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax.interpreters.ad.JVPTracer'>\n"
     ]
    }
   ],
   "source": [
    "dydx = grad(f)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax.interpreters.partial_eval.DynamicJaxprTracer'>\n"
     ]
    }
   ],
   "source": [
    "jf = jit(f)\n",
    "jit_dydx = jf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jitted functions\n",
    "\n",
    "* Currently, no function in GPJax is jitted. \n",
    "\n",
    "* Given everything is returned as function, I've left the decision about what needs jitting in the user's.\n",
    "    * Some guidance is given in notebooks.\n",
    "    \n",
    "* Thought-process here is that having lots of jitted code will make it tricky for users to debug their own code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other packages/frameworks used\n",
    "\n",
    "* [PyTest](https://docs.pytest.org/en/stable/) and [CodeCov](https://app.codecov.io/gh/thomaspinder/gpjax): Both fantastic. CodeCov can be a little finnicky, but it's really helpful so worth the odd frustration\n",
    "* [Sphinx](https://www.sphinx-doc.org/en/master/): Pain to setup, but just works (I hope!) afterwards\n",
    "* [ReadTheDocs](https://readthedocs.org/projects/gpjax/): Builds and hosts documentation in one click\n",
    "* [Black](https://black.readthedocs.io/en/stable/) and [iSort](https://github.com/PyCQA/isort): Formats code. Provided in a PR by someone else - seems to work fine though and salvages my messy code\n",
    "* [BumpVersion](https://pypi.org/project/bumpversion/) and [Twine](https://pypi.org/project/twine/): For semantic versioning and PyPi submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Next steps\n",
    "\n",
    "* Fully integrate the spectral kernel approximation i.e., [Sparse spectrum Gaussian processes](https://quinonero.net/Publications/lazaro-gredilla10a.pdf)\n",
    "* Now ObJax has been abstracted out, provide scope for Laplace approximations to the GP's latent variables when the likelihood is non-Gaussian\n",
    "* Provide an interface to NumPyro (work in progress)\n",
    "* Integrate work on graph kernels and GP-based dimensionality reduction\n",
    "\n",
    "![](graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Writing APIs in Jax is a net-positive experience\n",
    "    * Some mental gymnastics are involved with code structuring\n",
    "    * A more modular framework might (?) be more intuitive\n",
    "    * It makes decisions about how much abstraction to make easy\n",
    "* Jax lends itself quite nicely to multiple dispatch\n",
    "* The wider Jax ecosystem is already quite mature for a new-ish package"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
